{"name":"Topljets2015","tagline":"l+jets top analysis","body":"# TopLJets2015\r\n\r\n## Analysis twiki\r\nPlease keep \r\nhttps://twiki.cern.ch/twiki/bin/view/Main/TopLJ2015Analysis\r\nup to date with the on-going tasks and results\r\n\r\n## Installation instructions\r\nTo execute in your lxplus work area.\r\n```\r\ncmsrel CMSSW_7_4_14\r\ncd CMSSW_7_4_14/src\r\ncmsenv\r\ngit cms-merge-topic ikrav:egm_id_7.4.12_v1\r\ngit clone git@github.com:pfs/TopLJets2015.git\r\nscram b -j 9\r\n```\r\n\r\n## Running ntuple creation\r\nFirst time create a symbolic link to the jet energy corrections files\r\n```\r\nln -s data/Summer15_25nsV6_DATA.db\r\nln -s data/Summer15_25nsV6_MC.db\r\n```\r\nTo run locally the ntuplizer, for testing purposes\r\n```\r\ncmsRun test/runMiniAnalyzer_cfg.py runOnData=False/True outFilename=MiniEvents.root\r\n```\r\nTo submit a list of samples, described in a json file to the grid you can use the following script.\r\n```\r\npython scripts/submitToGrid.py -j data/samples_Run2015.json -c ${CMSSW_BASE}/src/TopLJets2015/TopAnalysis/test/runMiniAnalyzer_cfg.py --lfn my_output_directory_in_eos -s\r\n```\r\nPartial submission can be made adding \"-o csv_list\" as an option\r\nDon't forget to init the environment for crab3 (e.g. https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookCRAB3Tutorial).\r\n\r\nAs soon as ntuple production starts to finish, to move from crab output directories to a simpler directory structure which can be easily parsed by the local analysis run \r\n```\r\npython scripts/checkProductionIntegrity.py -i /store/group/phys_top/psilva/4a77bd2 -o /store/cmst3/user/psilva/LJets2015/5736a2c\r\n```\r\nIf \"--cleanup\" is passed, the original crab directories in EOS are removed.\r\n\r\n## Preparing the analysis \r\n\r\nAfter ntuples are processed start by creating the json files with the list of runs/luminosity sections processed, e.g. as:\r\n```\r\ncrab report grid/crab_Data13TeV_SingleElectron_2015D_v3\r\n``` \r\nThen you can merge the json files for the same dataset to get the full list of run/lumi sections to analyse\r\n```\r\nmergeJSON.py grid/crab_Data13TeV_SingleElectron_2015D_v3/results/lumiSummary.json grid/crab_Data13TeV_SingleElectron_2015D_v4/results/lumiSummary.json --output data/SingleElectron_lumiSummary.json\r\n```\r\nYou can then run the brilcalc tool to get the integrated luminosity in total and per run (see https://twiki.cern.ch/twiki/bin/view/CMS/2015LumiNormtag for more details).\r\n```\r\nbrilcalc lumi --normtag /afs/cern.ch/user/c/cmsbril/public/normtag_json/OfflineNormtagV1.json -i data/SingleElectron_lumiSummary.json\r\n```\r\nUse the table which is printed out to update the \"lumiPerRun\" method in ReadTree.cc.\r\nThat will be used to monitor the event yields per run in order to identify outlier runs.\r\n* Pileup weighting. To update the pileup distributions run the script below. It will store the data pileup distributions for different min.bias cross section in data/pileupWgts.root\r\n```\r\npython scripts/runPileupEstimation.py --json data/SingleElectron_lumiSummary.json\r\n```\r\n* B-tagging. To apply corrections to the simulation one needs the expected efficiencies stored somwewhere. The script below will project the jet pT spectrum from the TTbar sample before and after applying b-tagging, to compute the expecte efficiencies. The result will be stored in data/expTageff.root\r\n```\r\npython scripts/saveExpectedBtagEff.py \r\n```\r\n* MC normalization. This will loop over all the samples available in EOS and produce a normalization cache (weights to normalize MC). The file will be available in data/genweights.pck\r\n```\r\npython scripts/produceNormalizationCache.py -i /store/cmst3/user/psilva/LJets2015/5736a2c\r\n```\r\nYou're now ready to start locally the analysis.\r\n\r\n\r\n## Running locally the analysis\r\n\r\nThe analysis (histogram filling, final selection) is in src/ReadTree.cc.\r\nRecompile (scram b) everytime you change it so that you can test the new features.\r\nTo test the code on a single file to produce plots.\r\n```\r\npython scripts/runLocalAnalysis.py -i MiniEvents.root\r\n```\r\nTo run the code on a set of samples, listed in a json file you can run it as follows:\r\n```\r\npython scripts/runLocalAnalysis.py -i /store/cmst3/user/psilva/LJets2015/5736a2c -n 8 -o analysis_muplus   --ch 13   --charge 1\r\npython scripts/runLocalAnalysis.py -i /store/cmst3/user/psilva/LJets2015/5736a2c -n 8 -o analysis_muminus  --ch 13   --charge -1\r\npython scripts/runLocalAnalysis.py -i /store/cmst3/user/psilva/LJets2015/5736a2c -n 8 -o analysis_munoniso --ch 1300\r\n```\r\nThe first time it runs over the directory it will compute the normalization factor for MC\r\nsuch that the distributions will correspond to 1/pb of data.\r\nThe normalization factor is given by (xsec / N generated events)\r\nwhere xsec is stored in the json file, and N generated events is summed up\r\nfrom the \"counter\" histogram stored in the the files to process for each process.\r\nThe first time it also computes the pileup weights on a sample-by-sample basis\r\nby taking the ratio of the of the putrue distribution to the pileup distribution estimated in data.\r\nBoth the normalization factors and the pileup weights are stored under the \"analysis\" directory\r\nin a cache file called \".xsecweights.pck\".\r\nAfter the jobs have run you can merge the outputs with\r\n```\r\n./scripts/mergeOutputs.py analysis_muplus\r\n./scripts/mergeOutputs.py analysis_muminus\r\n./scripts/mergeOutputs.py analysis_munoniso/\r\n```\r\nTo plot the output of the local analysis you can run the following:\r\n```\r\npython scripts/plotter.py -i analysis_muplus/ -j data/samples_Run2015.json -l 2093.6\r\npython scripts/plotter.py -i analysis_muminus/ -j data/samples_Run2015.json -l 2093.6\r\npython scripts/plotter.py -i analysis_munoniso/ -j data/samples_Run2015.json -l 2093.6\r\n```\r\nAfter the plotters are created one can run the QCD estimation normalization, by fitting the MET distribution.\r\nThe script will also produce the QCD templates using the data from the sideband region. It runs as\r\n```\r\npython scripts/runQCDEstimation.py --iso analysis_muplus/plots/plotter.root --noniso analysis_munoniso/plots/plotter.root --out analysis_muplus/\r\npython scripts/runQCDEstimation.py --iso analysis_muminus/plots/plotter.root --noniso analysis_munoniso/plots/plotter.root --out analysis_muminus/\r\n```\r\nThe output is a ROOT file called Data_QCDMultijets.root which can now be used in addition to the predictions of all the other backgrounds.\r\nTo include it in the final plots you can run the plotter script again (see instructions above).\r\n\r\n## Cross section fitting\r\nWe use the Higgs combination tool to perform the fit of the production cross section.\r\n(cf. https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideHiggsAnalysisCombinedLimit for details of the release to use).\r\nIt currently has to be run from a CMSSW_7_1_5 release.\r\nTo create the datacard you can run the following script\r\n```\r\npython scripts/createDataCard.py -i analysis_muplus/plots/plotter.root -o analysis_muplus/datacard -d njetsnbtags\r\npython scripts/createDataCard.py -i analysis_muminus/plots/plotter.root -o analysis_muminus/datacard -d njetsnbtags\r\n```\r\nCombine the two datacards above into the final one\r\n```\r\nmkdir -p analysis_mu/datacard\r\ncd analysis_mu/datacard\r\ncombineCards.py muplus=../../analysis_muplus/datacard/datacard.dat muminus=../../analysis_muminus/datacard/datacard.dat > datacard.dat\r\ncd -\r\n```\r\nRun the fits and show the results\r\n```\r\npython scripts/fitCrossSection.py \"#mu^{+}\"=analysis_muplus/datacard/datacard.dat -o analysis_muplus/datacard\r\npython scripts/fitCrossSection.py \"#mu^{-}\"=analysis_muminus/datacard/datacard.dat -o analysis_muminus/datacard\r\npython scripts/fitCrossSection.py \"#mu^{#pm}\"=analysis_mu/datacard/datacard.dat -o analysis_mu/datacard\r\n```\r\nAfter all is run you can also compare the results with\r\n```\r\npython scripts/fitCrossSection.py \"#mu^{+}\"=analysis_muplus/datacard/datacard.dat  \"#mu^{-}\"=analysis_muminus/datacard/datacard.dat  \"#mu^{#pm}\"=analysis_mu/datacard/datacard.dat --noFit\r\n```\r\n\r\n\r\n## Updating the code\r\n\r\nCommit your changes regularly with\r\n```\r\ngit commit -a -m'comment on the changes made'\r\n```\r\nPush to your forked repository\r\n```\r\ngit push git@github.com:MYGITHUBLOGIN/TopLJets2015.git\r\n```\r\nFrom the github area of the repository cleak on the green button \"Compare,review and create a pull request\"\r\nto create the PR to merge with your colleagues.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}